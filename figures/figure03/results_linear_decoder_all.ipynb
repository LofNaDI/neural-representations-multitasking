{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import multitask.dataset as dataset\n",
    "from multitask.models.task_switching import get_task_model\n",
    "import multitask.models.task_switching.hooks as hooks\n",
    "from multitask.utils.training import get_device\n",
    "from multitask.utils.argparse import check_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = os.path.join('..', '..', 'data')\n",
    "path_pickle = os.path.join('pickle', 'results_linear_decoder_all.pickle')\n",
    "path_model_task_switching = os.path.join('..', '..', 'results', 'task_switching')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 10\n",
    "initial_seed = 6789\n",
    "max_seed = 10e5\n",
    "num_epochs = 50\n",
    "num_hidden = 10 * [100]\n",
    "batch_size = 100\n",
    "num_train = 50000\n",
    "num_test = 10000\n",
    "tasks_names = ['parity', 'value']\n",
    "idxs_contexts = list(range(len(num_hidden)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found simulation in ../../results/task_switching with the same parameters (2022-09-28_02_23_14)\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'num_runs': num_runs,\n",
    "    'initial_seed': initial_seed,\n",
    "    'max_seed': max_seed,\n",
    "    'num_epochs': num_epochs,\n",
    "    'num_hidden': num_hidden,\n",
    "    'batch_size': batch_size,\n",
    "    'num_train': num_train,\n",
    "    'num_test': num_test,\n",
    "    'tasks': tasks_names,\n",
    "    'idxs_contexts': idxs_contexts\n",
    "}\n",
    "\n",
    "data_folder = check_runs(path_model_task_switching, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_data = os.path.join(data_folder, 'data.pickle')\n",
    "with open(pickle_data, 'rb') as handle:\n",
    "    results_task_switching = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10612, 17350, 130146, 173249, 213794, 341996, 440064, 668870, 858781, 894813]\n",
      "['parity', 'value']\n"
     ]
    }
   ],
   "source": [
    "seeds = sorted(list(results_task_switching.keys()))\n",
    "num_seeds = len(seeds)\n",
    "num_tasks = len(tasks_names)\n",
    "\n",
    "print(seeds)\n",
    "print(tasks_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sgalella/miniconda3/envs/multitask/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /tmp/pip-req-build-ex__3qls/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parity: [1, 0]\n",
      "value: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "tasks_datasets = dataset.get_tasks_dict(tasks_names, root=path_data)\n",
    "\n",
    "task_switching_tasks = {}\n",
    "num_tasks = len(tasks_names)\n",
    "\n",
    "for i_context, task_name in enumerate(tasks_names):\n",
    "    task_switching_tasks[task_name] = {}\n",
    "    task_switching_tasks[task_name]['data'] = tasks_datasets[task_name]\n",
    "    task_switching_tasks[task_name]['activations'] = num_tasks * [0]\n",
    "    task_switching_tasks[task_name]['activations'][i_context] = 1  # Set to 0 for Removed\n",
    "\n",
    "for key, value in task_switching_tasks.items():\n",
    "    print(f'{key}: {value[\"activations\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.81it/s]\n"
     ]
    }
   ],
   "source": [
    "device = get_device()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "seeds_task_swithing  = sorted(list(results_task_switching.keys()))\n",
    "list_activations = []\n",
    "list_numbers = []\n",
    "\n",
    "for i_seed, seed in tqdm(enumerate(seeds_task_swithing), total=num_runs):\n",
    "    state_dict = results_task_switching[seed]['model']\n",
    "    model = get_task_model(task_switching_tasks,\n",
    "                           num_hidden,\n",
    "                           idxs_contexts,\n",
    "                           device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    indices = results_task_switching[seed]['indices']\n",
    "\n",
    "    test_sampler = dataset.SequentialSampler(indices['test'])\n",
    "    _, test_dataloaders = dataset.create_dict_dataloaders(task_switching_tasks,\n",
    "                                                          indices,\n",
    "                                                          batch_size=batch_size)\n",
    "    tasks_testloader = dataset.SequentialTaskDataloader(test_dataloaders)\n",
    "\n",
    "    numbers = test_dataloaders[tasks_names[0]].dataset.numbers.numpy()\n",
    "    numbers = numbers[indices['test']]\n",
    "\n",
    "    _, activations = hooks.get_layer_activations(model,\n",
    "                                                tasks_testloader,\n",
    "                                                criterion,\n",
    "                                                device=device,\n",
    "                                                disable=True)\n",
    "    \n",
    "    list_activations.append(activations)\n",
    "    list_numbers.append(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 100%|██████████| 10/10 [01:09<00:00,  6.94s/it]\n",
      "1: 100%|██████████| 10/10 [01:10<00:00,  7.01s/it]\n",
      "2: 100%|██████████| 10/10 [01:04<00:00,  6.42s/it]\n",
      "3: 100%|██████████| 10/10 [00:51<00:00,  5.16s/it]\n",
      "4: 100%|██████████| 10/10 [01:00<00:00,  6.09s/it]\n",
      "5: 100%|██████████| 10/10 [01:04<00:00,  6.48s/it]\n",
      "6: 100%|██████████| 10/10 [01:13<00:00,  7.37s/it]\n",
      "7: 100%|██████████| 10/10 [01:13<00:00,  7.38s/it]\n",
      "8: 100%|██████████| 10/10 [01:10<00:00,  7.04s/it]\n",
      "9: 100%|██████████| 10/10 [01:03<00:00,  6.39s/it]\n"
     ]
    }
   ],
   "source": [
    "num_layers = len(num_hidden)\n",
    "max_iter = 8000\n",
    "\n",
    "acc_numbers_all = np.zeros((num_seeds, num_layers))\n",
    "acc_tasks_all = np.zeros((num_seeds, num_layers))\n",
    "acc_congruency_all = np.zeros((num_seeds, num_layers))\n",
    "\n",
    "for i_seed, seed in enumerate(seeds):\n",
    "    activations = list_activations[i_seed]\n",
    "    numbers = list_numbers[i_seed]\n",
    "\n",
    "    labels_numbers = np.hstack((numbers, numbers))\n",
    "    labels_task = np.concatenate((np.zeros_like(numbers), np.ones_like(numbers)))\n",
    "    labels_congruency = np.array([1 if number in [0, 2, 4, 5, 7, 9] else 0 for number in labels_numbers])\n",
    "\n",
    "    for j_layer in tqdm(range(num_layers), desc=f'{i_seed}'):\n",
    "        activations_decoder = None\n",
    "        for task in tasks_names:\n",
    "            activations_task = activations[task][f'layer{j_layer+1}']\n",
    "            if activations_decoder is None:\n",
    "                activations_decoder = activations_task\n",
    "            else:\n",
    "                activations_decoder = np.vstack((activations_decoder, \n",
    "                                                activations_task))\n",
    "        assert activations_decoder.shape[0] == labels_numbers.shape[0]\n",
    "\n",
    "        activations_decoder = (activations_decoder - activations_decoder.mean()) / activations_decoder.std()\n",
    "\n",
    "        # Numbers task\n",
    "        seed = np.random.randint(0, 1e8, 1)[0]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(activations_decoder,\n",
    "                                                            labels_numbers,\n",
    "                                                            test_size=0.1,\n",
    "                                                            random_state=seed)\n",
    "        clf = LogisticRegression(random_state=seed,\n",
    "                                max_iter=max_iter,\n",
    "                                tol=1e-3).fit(X_train, y_train)\n",
    "        acc_numbers_all[i_seed, j_layer] = clf.score(X_test, y_test)\n",
    "\n",
    "        # Labels task\n",
    "        seed = np.random.randint(0, 1e8, 1)[0]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(activations_decoder,\n",
    "                                                            labels_task,\n",
    "                                                            test_size=0.1,\n",
    "                                                            random_state=seed)\n",
    "        clf = LogisticRegression(random_state=seed,\n",
    "                                max_iter=max_iter,\n",
    "                                tol=1e-3).fit(X_train, y_train)\n",
    "        acc_tasks_all[i_seed, j_layer] = clf.score(X_test, y_test)\n",
    "\n",
    "        # Congruency task\n",
    "        seed = np.random.randint(0, 1e8, 1)[0]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(activations_decoder,\n",
    "                                                            labels_congruency,\n",
    "                                                            test_size=0.1,\n",
    "                                                            random_state=seed)\n",
    "        clf = LogisticRegression(random_state=seed,\n",
    "                                max_iter=max_iter,\n",
    "                                tol=1e-3).fit(X_train, y_train)\n",
    "        acc_congruency_all[i_seed, j_layer] = clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "results['numbers'] = acc_numbers_all\n",
    "results['tasks'] = acc_tasks_all\n",
    "results['congruency'] = acc_congruency_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_pickle, 'wb') as f:\n",
    "        pickle.dump(results, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5de0b3d16828453b801d3a971a2e845298ac67ea708b1fd16f0d1197d2abd69f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

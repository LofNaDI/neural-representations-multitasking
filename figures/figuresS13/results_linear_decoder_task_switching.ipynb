{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import multitask.dataset as dataset\n",
    "from multitask.models.task_switching import get_task_model\n",
    "import multitask.models.task_switching.hooks as hooks\n",
    "from multitask.utils.training import get_device\n",
    "from multitask.utils.argparse import check_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = os.path.join('..', '..', 'data')\n",
    "path_pickle = os.path.join('pickle', 'results_linear_decoder_task_switching.pickle')\n",
    "path_model_task_switching = os.path.join('..', '..', 'results', 'task_switching')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 10\n",
    "initial_seed = 1234\n",
    "max_seed = 10e5\n",
    "num_epochs = 50\n",
    "num_hidden = 10 * [100]\n",
    "batch_size = 100\n",
    "num_train = 41080\n",
    "num_test = 8216\n",
    "tasks_names = ['vowel', 'position']\n",
    "idxs_contexts = list(range(len(num_hidden)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found simulation in ../../results/task_switching with the same parameters (2024-01-12_18-56-09)\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'num_runs': num_runs,\n",
    "    'initial_seed': initial_seed,\n",
    "    'max_seed': max_seed,\n",
    "    'num_epochs': num_epochs,\n",
    "    'num_hidden': num_hidden,\n",
    "    'batch_size': batch_size,\n",
    "    'num_train': num_train,\n",
    "    'num_test': num_test,\n",
    "    'tasks': tasks_names,\n",
    "    'idxs_contexts': idxs_contexts\n",
    "}\n",
    "\n",
    "data_folder = check_runs(path_model_task_switching, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_data = os.path.join(data_folder, 'data.pickle')\n",
    "with open(pickle_data, 'rb') as handle:\n",
    "    results_task_switching = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[165158, 220532, 318129, 451283, 486191, 514041, 818831, 869016, 908341, 978124]\n",
      "['vowel', 'position']\n"
     ]
    }
   ],
   "source": [
    "seeds = sorted(list(results_task_switching.keys()))\n",
    "num_seeds = len(seeds)\n",
    "num_tasks = len(tasks_names)\n",
    "\n",
    "print(seeds)\n",
    "print(tasks_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vowel: [1, 0]\n",
      "position: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "tasks_datasets = dataset.get_tasks_dict(tasks_names, root=path_data)\n",
    "\n",
    "task_switching_tasks = {}\n",
    "num_tasks = len(tasks_names)\n",
    "\n",
    "for i_context, task_name in enumerate(tasks_names):\n",
    "    task_switching_tasks[task_name] = {}\n",
    "    task_switching_tasks[task_name]['data'] = tasks_datasets[task_name]\n",
    "    task_switching_tasks[task_name]['activations'] = num_tasks * [0]\n",
    "    task_switching_tasks[task_name]['activations'][i_context] = 1  # Set to 0 for Removed\n",
    "\n",
    "for key, value in task_switching_tasks.items():\n",
    "    print(f'{key}: {value[\"activations\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809b19e1eb74403c8883c7491819a469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = get_device()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "seeds_task_swithing  = sorted(list(results_task_switching.keys()))\n",
    "list_activations = []\n",
    "list_letters = []\n",
    "\n",
    "for i_seed, seed in tqdm(enumerate(seeds_task_swithing), total=num_runs):\n",
    "    state_dict = results_task_switching[seed]['model']\n",
    "    model = get_task_model(task_switching_tasks,\n",
    "                           num_hidden,\n",
    "                           idxs_contexts,\n",
    "                           device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    indices = results_task_switching[seed]['indices']\n",
    "\n",
    "    test_sampler = dataset.SequentialSampler(indices['test'])\n",
    "    _, test_dataloaders = dataset.create_dict_dataloaders(task_switching_tasks,\n",
    "                                                          indices,\n",
    "                                                          batch_size=batch_size)\n",
    "    tasks_testloader = dataset.SequentialTaskDataloader(test_dataloaders)\n",
    "\n",
    "    letters = test_dataloaders[tasks_names[0]].dataset.letters.numpy()\n",
    "    letters = letters[indices['test']]\n",
    "\n",
    "    _, activations = hooks.get_layer_activations(model,\n",
    "                                                tasks_testloader,\n",
    "                                                criterion,\n",
    "                                                device=device,\n",
    "                                                disable=True)\n",
    "    \n",
    "    list_activations.append(activations)\n",
    "    list_letters.append(letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48790710fedc47e281fa0f11078a5d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 [165158]:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5909541e9564e5c8d35d3d22d732bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1 [220532]:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3665a0f6e8e448cbd7d3039a7057094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2 [318129]:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e1e53fbecc4e71ad4fa6637f52220b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "3 [451283]:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da80abb221d48549d35b565692744d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "4 [486191]:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe466590cc5a49b3b3fd37502504ae77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "5 [514041]:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c25a17f2cd8249c4a39fc564cb49b18c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "6 [818831]:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bfcd938259b4107a30b51b7bb75d5bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "7 [869016]:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd81f747e3c4ee588de895fe35d00f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "8 [908341]:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7b4f23ddea54dad998a7b00d22b90d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "9 [978124]:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_layers = len(num_hidden)\n",
    "max_iter = 8000\n",
    "\n",
    "acc_letters_all = np.zeros((num_seeds, num_layers))\n",
    "acc_tasks_all = np.zeros((num_seeds, num_layers))\n",
    "acc_congruency_all = np.zeros((num_seeds, num_layers))\n",
    "acc_output_all = np.zeros((num_seeds, num_layers))\n",
    "\n",
    "\n",
    "for i_seed, seed in enumerate(seeds):\n",
    "    activations = list_activations[i_seed]\n",
    "    letters = list_letters[i_seed]\n",
    "\n",
    "    labels_letters = np.hstack((letters, letters))\n",
    "    labels_task = np.concatenate((np.zeros_like(letters), np.ones_like(letters)))\n",
    "    labels_congruency = np.array([1 if letter in [0, 4, 8, 14, 20, 13, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25] else 0 for letter in labels_letters])\n",
    "\n",
    "    labels_output_vowel = np.array([1 if letter in [0, 4, 8, 14, 20] else 0 for letter in letters])\n",
    "    labels_output_position = np.array([1 if letter < 13 else 0 for letter in letters])\n",
    "    labels_output = np.concatenate((labels_output_vowel, labels_output_position))\n",
    "\n",
    "    for j_layer in tqdm(range(num_layers), desc=f'{i_seed} [{seed}]'):\n",
    "        activations_decoder = None\n",
    "        for task in tasks_names:\n",
    "            activations_task = activations[task][f'layer{j_layer+1}']\n",
    "            if activations_decoder is None:\n",
    "                activations_decoder = activations_task\n",
    "            else:\n",
    "                activations_decoder = np.vstack((activations_decoder, \n",
    "                                                activations_task))\n",
    "        assert activations_decoder.shape[0] == labels_letters.shape[0]\n",
    "\n",
    "        activations_decoder = (activations_decoder - activations_decoder.mean()) / activations_decoder.std()\n",
    "\n",
    "        # Letters task\n",
    "        X_train, X_test, y_train, y_test = train_test_split(activations_decoder,\n",
    "                                                            labels_letters,\n",
    "                                                            test_size=0.1,\n",
    "                                                            random_state=seed)\n",
    "        clf = LogisticRegression(random_state=seed,\n",
    "                                max_iter=max_iter,\n",
    "                                tol=1e-3).fit(X_train, y_train)\n",
    "        acc_letters_all[i_seed, j_layer] = clf.score(X_test, y_test)\n",
    "\n",
    "        # Labels task\n",
    "        X_train, X_test, y_train, y_test = train_test_split(activations_decoder,\n",
    "                                                            labels_task,\n",
    "                                                            test_size=0.1,\n",
    "                                                            random_state=seed)\n",
    "        clf = LogisticRegression(random_state=seed,\n",
    "                                max_iter=max_iter,\n",
    "                                tol=1e-3).fit(X_train, y_train)\n",
    "        acc_tasks_all[i_seed, j_layer] = clf.score(X_test, y_test)\n",
    "\n",
    "        # Congruency task\n",
    "        X_train, X_test, y_train, y_test = train_test_split(activations_decoder,\n",
    "                                                            labels_congruency,\n",
    "                                                            test_size=0.1,\n",
    "                                                            random_state=seed)\n",
    "        clf = LogisticRegression(random_state=seed,\n",
    "                                max_iter=max_iter,\n",
    "                                tol=1e-3).fit(X_train, y_train)\n",
    "        acc_congruency_all[i_seed, j_layer] = clf.score(X_test, y_test)\n",
    "\n",
    "        # Output task\n",
    "        X_train, X_test, y_train, y_test = train_test_split(activations_decoder,\n",
    "                                                            labels_output,\n",
    "                                                            test_size=0.1,\n",
    "                                                            random_state=seed)\n",
    "        clf = LogisticRegression(random_state=seed,\n",
    "                                max_iter=max_iter,\n",
    "                                tol=1e-3).fit(X_train, y_train)\n",
    "        acc_output_all[i_seed, j_layer] = clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "results['letters'] = acc_letters_all\n",
    "results['tasks'] = acc_tasks_all\n",
    "results['congruency'] = acc_congruency_all\n",
    "results['output'] = acc_output_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_pickle, 'wb') as f:\n",
    "        pickle.dump(results, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "5de0b3d16828453b801d3a971a2e845298ac67ea708b1fd16f0d1197d2abd69f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
